{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wnnm(img, patchRadius, delta, c, K, sigma_n,N_threshold):\n",
    "    # This function applies weighted nuclear norm minimization based denoising to the imput image img\n",
    "    # Specify the search window\n",
    "    searchWindowRadius = patchRadius*3\n",
    "    \n",
    "    # Specify the number of iterations for estimating \\hat{X}_j\n",
    "    N_iter = 3\n",
    "    \n",
    "    # Specify the width of padding and pad the noisy image\n",
    "    pad = searchWindowRadius + patchRadius\n",
    "    imgPad = np.pad(img, pad_width = pad)\n",
    "    imgPad = imgPad[..., pad:-pad]\n",
    "    \n",
    "    # Initialize variables to be iterated over\n",
    "    xhat_iter = img\n",
    "    \n",
    "    for n in range(K):\n",
    "        # Pad the image for the iteration\n",
    "        xhat_iter = np.pad(xhat_iter, pad_width = pad)\n",
    "        xhat_iter = xhat_iter[..., pad:-pad] ## remove\n",
    "        \n",
    "        # Regularize the image that is denoised during the iteration\n",
    "        y_iter = xhat_iter + delta*(imgPad - xhat_iter)\n",
    "        \n",
    "        # Initialize the matrix to keep track of how many times each pixel has been updated\n",
    "        pixel_contribution_matrix = np.ones_like(imgPad)\n",
    "        \n",
    "        # Identify similar patches and produce the matrix of similar patches\n",
    "        for j in range(img.shape[0]):\n",
    "            for i in range(img.shape[1]):\n",
    "                # Select the central patch\n",
    "                centerPatch = y_iter[j+searchWindowRadius:j+searchWindowRadius+2*patchRadius,\n",
    "                                 i+searchWindowRadius:i+searchWindowRadius+2*patchRadius,\n",
    "                                 :]\n",
    "                \n",
    "                # Initialize the vector of distances between patches \n",
    "                dists= np.ones(((2*searchWindowRadius+1)**2))\n",
    "                # Initialize the matrix of patches\n",
    "                patches = np.zeros(((2*searchWindowRadius+1)**2,(2*patchRadius)**2))\n",
    "                # Compute distances between patches\n",
    "                # This is partially vectorized by using indexing to take out patches in a sliding window fashing \n",
    "                # out of a vertical slice through the search window\n",
    "                for k in range(2*searchWindowRadius+1):\n",
    "                    # Take a vertical slice in the search window\n",
    "                    otherPatch = y_iter[j:j+2*pad,\n",
    "                                    i+k:i+k+2*patchRadius,\n",
    "                                    :]\n",
    "                    \n",
    "                    # Determine indices corresponding to patches in a window sliding down the search window\n",
    "                    indexer = np.arange((2*patchRadius)**2)[None, :] + (2*patchRadius)*np.arange(otherPatch.shape[0]-2*patchRadius+1)[:, None]\n",
    "                    \n",
    "                    # Set columns to be patches\n",
    "                    otherPatch = otherPatch.flatten()\n",
    "                    otherPatch = np.reshape(otherPatch[indexer],(otherPatch[indexer].shape[0],(2*patchRadius)**2))\n",
    "                    \n",
    "                    # Compute distance and store the corresponding patches\n",
    "                    dists[k*(2*searchWindowRadius+1):(k+1)*(2*searchWindowRadius+1)] = (np.sum((centerPatch.reshape(((2*patchRadius)**2))-otherPatch)**2,axis=1)/(2*patchRadius)**2).flatten()\n",
    "                    patches[k*(2*searchWindowRadius+1):(k+1)*(2*searchWindowRadius+1),:] = otherPatch\n",
    "                    \n",
    "                # Select to N_threshold nearest patches and creat a patch matrix\n",
    "                indcs = np.argsort(dists)\n",
    "                Yj = (patches[indcs[:N_threshold],:]).transpose()\n",
    "                \n",
    "                # Center the columns\n",
    "                Yj_means = np.sum(Yj,axis=0)\n",
    "                Yj_center = Yj - Yj_means\n",
    "                \n",
    "                # First iteration need to estimate singular values of Xj\n",
    "                U,S,V_T = svd(Yj_center, full_matrices=False)\n",
    "                sing_val = np.sqrt(np.maximum(S**2-N_threshold*sigma_n**2,0))\n",
    "                \n",
    "                # Calculate the weights and sinfular values of \\hat{X}_j iteratively\n",
    "                for m in range(N_iter):\n",
    "                    w = c*np.sqrt(N_threshold)/(sing_val+10**(-6))\n",
    "                    sing_val = np.diag(np.maximum(S-w,0))\n",
    "                \n",
    "                # Compute \\hat{X}_j\n",
    "                Xj_hat_center = U@np.diag(np.maximum(S-w,0))@V_T\n",
    "                Xj_hat = Xj_hat_center + Yj_means\n",
    "                \n",
    "                # Add the estimate of denoised central patch (first column of \\hat{X}_j) to the esmated denoised image clipping it to between 0 and 1\n",
    "                xhat_iter[j+searchWindowRadius:j+searchWindowRadius+2*patchRadius,\n",
    "                                 i+searchWindowRadius:i+searchWindowRadius+2*patchRadius,\n",
    "                                 :] = xhat_iter[j+searchWindowRadius:j+searchWindowRadius+2*patchRadius,\n",
    "                                 i+searchWindowRadius:i+searchWindowRadius+2*patchRadius,\n",
    "                                 :] + np.clip(Xj_hat[:,0].reshape((2*patchRadius,2*patchRadius,1)),0,1)\n",
    "                \n",
    "                # Keep track of how many times each pixel has been added to\n",
    "                pixel_contribution_matrix[j+searchWindowRadius:j+searchWindowRadius+2*patchRadius,\n",
    "                                 i+searchWindowRadius:i+searchWindowRadius+2*patchRadius,\n",
    "                                 :] = pixel_contribution_matrix[j+searchWindowRadius:j+searchWindowRadius+2*patchRadius,\n",
    "                                 i+searchWindowRadius:i+searchWindowRadius+2*patchRadius,\n",
    "                                 :] + np.ones_like(pixel_contribution_matrix[j+searchWindowRadius:j+searchWindowRadius+2*patchRadius,\n",
    "                                 i+searchWindowRadius:i+searchWindowRadius+2*patchRadius,:])\n",
    "        \n",
    "        # Remove the padding and average out contributions to pixels from different patches\n",
    "        xhat_iter = xhat_iter[pad:-pad,\n",
    "                    pad:-pad,\n",
    "                   :]/pixel_contribution_matrix[pad:-pad,\n",
    "                    pad:-pad,\n",
    "                   :]\n",
    "        \n",
    "    # Produce the final output\n",
    "    out = xhat_iter\n",
    "    return out\n",
    "                    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def WNNM(img, patchSize, delta, c, K, sigma_n, nThreshold):\n",
    "    windowSize = 5 * patchSize\n",
    "    pad = windowSize\n",
    "    paddedImg = np.pad(img, pad_width=pad)\n",
    "    X_hat = img\n",
    "    for n in range(K):\n",
    "        X_hat = np.pad(X_hat, pad_width=pad)\n",
    "        Y_hat = X_hat + delta * (X_hat - paddedImg)\n",
    "        counting_update_numbers = np.ones_like(paddedImg)\n",
    "        for i in range(windowSize, paddedImg.shape[0] - windowSize):\n",
    "            for j in range(windowSize, paddedImg.shape[1] - windowSize):\n",
    "                window = Y_hat[i - windowSize:i + windowSize + 1, j - windowSize:j + windowSize + 1]\n",
    "                \n",
    "                # Vectorized main patch selection\n",
    "                mainPatch = Y_hat[i - patchSize:i + patchSize + 1, j - patchSize:j + patchSize + 1]\n",
    "                \n",
    "                # Vectorized window slicing and reshaping\n",
    "                window_vectorized = window.reshape(-1, window.shape[-1])\n",
    "                \n",
    "                # Calculate the squared difference between mainPatch and each patch in the window\n",
    "                patch_diff = (mainPatch - window_vectorized[:, None, None]) / 255\n",
    "                patch_diff_sq = np.sum(patch_diff ** 2, axis=(1, 2))\n",
    "                \n",
    "                # Reshape the squared differences to match the window shape\n",
    "                patch_diff_sq_reshaped = patch_diff_sq.reshape(window.shape[:-1])\n",
    "                \n",
    "                # Find the indices of the smallest distances\n",
    "                sorted_indices = np.argsort(patch_diff_sq_reshaped.flatten())\n",
    "                threshold_indices = sorted_indices[:nThreshold]\n",
    "                print(threshold_indices)\n",
    "\n",
    "                # print(threshold_indices)\n",
    "                vectorized_patches = []\n",
    "                for idx in threshold_indices:\n",
    "                    patch = window[idx - patchSize:idx + patchSize + 1, patchSize:2 * patchSize + 1]\n",
    "                    print(patch)\n",
    "                    vectorized_patch = patch.flatten()\n",
    "                    print(vectorized_patch)\n",
    "                    vectorized_patches.append(vectorized_patch)\n",
    "                patches_matrix = np.column_stack(vectorized_patches)\n",
    "\n",
    "                # Step 1: Singular Value Decomposition\n",
    "                U, Sigma, V = np.linalg.svd(patches_matrix, full_matrices=False)\n",
    "                # print(f'{U.shape},{Sigma.shape}, {V.shape}')\n",
    "                # Step 2: Calculate weight matrix\n",
    "                num_cols = patches_matrix.shape[1]\n",
    "                weights = c * np.sqrt(num_cols) / (Sigma + 1e-16)\n",
    "                weights = np.sort(weights)[::-1]  # Sort weights in non-descending order\n",
    "\n",
    "                # Step 3: Update counting matrix\n",
    "                counting_update_numbers[i - patchSize:i + patchSize + 1, j - patchSize:j + patchSize + 1] += 1\n",
    "\n",
    "                # Step 4: Reconstruct main patch\n",
    "                Sigma_hat = np.diag(np.maximum(Sigma - weights, 0))\n",
    "\n",
    "                # Adjust dimensions of U, Sigma_hat, and V.T if necessary\n",
    "                if Sigma_hat.shape[0] < U.shape[1]:\n",
    "                    U = U[:, :Sigma_hat.shape[0]]\n",
    "                if Sigma_hat.shape[0] < V.shape[0]:\n",
    "                    V = V.T[:Sigma_hat.shape[0], :]\n",
    "\n",
    "                reconstructed_patch = np.matmul(U, np.matmul(Sigma_hat, V))\n",
    "\n",
    "                # Step 5: Update X_hat\n",
    "                X_hat[i - patchSize:i + patchSize + 1, j - patchSize:j + patchSize + 1] = reconstructed_patch\n",
    "\n",
    "        # Step 6: Normalize X_hat\n",
    "        X_hat = X_hat[pad:-pad, pad:-pad]\n",
    "        X_hat = X_hat / counting_update_numbers\n",
    "\n",
    "    return X_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WNNM(img,patchSize,c,k,delta,sigma,nThreshold):\n",
    "    windowSize=3*patchSize\n",
    "    paddedImg=np.pad(img,pad_width=windowSize,mode='constant')\n",
    "    numberOfIt=3\n",
    "    x_hat=paddedImg\n",
    "    for n in range(K):\n",
    "        y_hat=x_hat+delta*(paddedImg - x_hat)\n",
    "        pixel_contribution_matrix = np.ones_like(paddedImg)\n",
    "        for i in range(windowSize, img.shape[0]-windowSize):\n",
    "            for j in range(windowSize,img.shape[1]-windowSize):\n",
    "                window=y_hat[i - windowSize:i+windowSize + 1,j-windowSize : j+windowSize+1]\n",
    "                mainPatch=y_hat[i -patchSize:i+patchSize + 1,j-patchSize : j+patchSize+1]\n",
    "                distances=[]\n",
    "                for k in range(patchSize,windowSize-patchSize):\n",
    "                    distance=[]\n",
    "                    for l in range(patchSize,windowSize-patchSize):\n",
    "                        similarPatch=window[k-patchSize:k+patchSize+1,l-patchSize:l+patchSize+1]\n",
    "                        dis=np.sum((mainPatch-similarPatch)**2)\n",
    "                        distance.append(dis)\n",
    "                    distances.append(distance)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
